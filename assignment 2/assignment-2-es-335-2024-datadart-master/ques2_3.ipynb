{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Refer to the instructor's notebook on multi-colinearity. Use np.linalg.solve instead of np.linalg.inv for the same problem. Compare and contrast their usage, which one is better and why? [1 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "The normal equation for linear regression exists as:\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "In the original notebook, we used the 'np.linalg.inv' function calculate the inverse which might be computationally expensive and less stable for large matrices. Thus, we may also use 'np.linalg.solve' method directly to solve the above system of linear equations.\n",
    "\n",
    "The code for the same will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix is singular\n",
      "X.T @ X = \n",
      " [[ 3.  6. 12.]\n",
      " [ 6. 14. 28.]\n",
      " [12. 28. 56.]]\n",
      "Theta using np.linalg.solve: None\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = 2*x1\n",
    "y = np.array([4, 6, 8])\n",
    "all_ones = np.ones(x1.shape[0])\n",
    "X = np.array([all_ones, x1, x2]).T\n",
    "\n",
    "def solve_normal_equation(X, y):\n",
    "    try:\n",
    "        theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        return theta\n",
    "    except np.linalg.LinAlgError:\n",
    "        print('The matrix is singular')\n",
    "        print(\"X.T @ X = \\n\", X.T @ X)\n",
    "        return None\n",
    "\n",
    "theta = solve_normal_equation(X, y)\n",
    "print(\"Theta using np.linalg.solve:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to compare both the functions and their utilities, 'np.linalg.inv' method involves the calculation of matrix inverse and thus has a time complexity of the order of O(n^3), where n is the size of the matrix. This is computationally expensive for large matrices, as it requires more operations.\n",
    "Whereas for 'np.linalg.solve' method, the system of linear equations is solved directly using the specialized algorithm, LU Decomposition, which lowers the time complexity than matrix inversion. \n",
    "\n",
    "Also, the process of inverting a matrix may lead to numerical instability in cases of a singular matrix, i.e., when the determinant of the matrix is zero or approaches zero. Whereas there is no such case in the 'np.linalg.solve' method unlike the 'np.linalg.inv' method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Referring to the same notebook, explain why Sklearn's linear regression implementation is robust against multicollinearity. Dive deep into Sklearn's code and explain in depth the methodology used in sklearn's implementation. [1 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "Sklearn's LinearRegression uses a more advanced algorithm called the Ordinary Least Squares (OLS) method, similar to the normal equation, but with additional enhancements. It internally handles the issue of multicollinearity by employing Singular Value Decomposition (SVD).\n",
    "\n",
    "The goal of linear regression is to find the coefficients θ that minimize the sum of squared differences between the predicted values and the actual values. This is typically formulated as the least squares problem.\n",
    "The ordinary least squares solution involves solving the system of equations: $$ X^TX\\theta = X^Ty $$ for θ, where X is the feature matrix and y is the target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD method decomposes the feature matrix X into three matrices U, Σ, and V, such that X = UΣV. The coefficients $\\theta$ are then calculated as $\\theta = V \\cdot \\text{diag}\\left(\\frac{1}{\\sigma_i}\\right) \\cdot U^T \\cdot y$, where $\\sigma_i$ are the singular values obtained from the diagonal of $\\Sigma$.\n",
    "The SVD approach is numerically stable, hence works even for ill-conditioned matrices, i.e. matrices whose ratio of largest to smallest singular value in the matrix is very high.\n",
    "\n",
    "The implementation includes checks for edge-cases, such as singular or nearly singular matrices which may lead to issues in the matrix inversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.4, 0.8]), 2.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "x1 = np.array([1, 2, 3])\n",
    "x2 = 2*x1\n",
    "data = np.array([x1, x2]).T\n",
    "y = np.array([4, 6, 8])\n",
    "lr.fit(data, y)\n",
    "lr.coef_, lr.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
